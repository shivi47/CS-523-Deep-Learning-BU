\documentclass{article}
\usepackage{graphicx}

\title{CS 523 Summer Semester 2, 2021: Problem Set 2}

\begin{document}
\maketitle
\textbf{Release Date}: Wednesday, July 21, 2021 at 23:59 EST\newline
\textbf{Due Date}: Monday, August 2, 2021 at 23:59 EST\newline
\textbf{Deliverables}: Your completed source code package in a \texttt{.zip} archive. Please remove any \texttt{\_\_pycache\_\_} directories. This will be submitted on Gradescope.\newline
\textbf{Reminder}: Please feel free to work together. If you collaborate with others, please write your team members names' in your submission.
\newline
\newline
Last time, we implemented a feed-forward layer (which is stackable) that has a small library of activation functions and a single loss function (and optimizer). Most importantly, the activation functions we implemented were element-wise \textbf{independent} functions, which makes their jacobean's (for a single example) diagonal. Thus, we can simply store only the diagonal and save us a headache.
\newline\newline
This time however, we will be dealing with element-wise \textbf{dependent} activation functions. Specifically, we will be dealing with the \texttt{softmax} function. Additionally, we will be adding loss functions that deal with probability distributions, mainly binary and categorical cross entropy.
\newline
\newline
There are aspects of this problem set which will be covered during lecture while this assignment is out, so don't worry if this looks scary at the moment! Also, I highly encourage you to visit office hours if you have questions!
\newline
\newline
I have again provided starter code again. Please copy the following files (preserving the existing directory structure) into your existing neural network package (i.e. ps1). The following code files need to be completed are:
\begin{enumerate}
    \item \texttt{nn/layers/softmax.py} (40pts)
    \item \texttt{nn/losses/bce.py} (30pts)
    \item \texttt{nn/losses/cross\_entropy.py} (30pts)
\end{enumerate}

The files \texttt{test\_bce.py} and \texttt{test\_categorical\_crossentropy.py} are your testing scripts. Please go ahead and run them in order to test your code. If the progress bar completes and you see the helpful message, your code works as expected!

\end{document}

